{"name":"Kaggle whatscooking","tagline":"Kaggle Competition - WhatsCooking - Multiclass Problem to predict cuisine type","body":"#Case Study on Kaggle _ Whats Cooking\r\nThis is a fun learning competition from kaggle on predicting the dish's cuisine type with the set of provided ingredients. \r\n##A  Brief Description of this Problem: \r\nIf you're in Northern California, you'll be walking past the inevitable bushels of leafy greens, spiked with dark purple kale and the bright pinks and yellows of chard. Across the world in South Korea, mounds of bright red kimchi greet you, while the smell of the sea draws your attention to squids squirming nearby. India’s market is perhaps the most colorful, awash in the rich hues and aromas of dozens of spices: turmeric, star anise, poppy seeds, and garam masala as far as the eye can see.\r\n         So, with the given list of ingredients, predicting the dish's cuisine is our challenge.\r\n\r\n## Data Analysis and Modelling.\r\n The training set is provided as JSON File which needs to be processed so it can be workable in R. The JSON file was converted in to a dataset using JSONLITE package functions. Several Text processing were done on this dataset to assign each ingredient as a variable in the dataset. Similar processing had to be done on the test set as well so the predicted models can be applied to the test set.On looking at the training dataset, the Italian cuisine has got maximum occurrence. This is just an initial picture of what the train set look like. With this, several models were applied on the data to find the accuracy of predictions.\r\n\r\n![](https://raw.githubusercontent.com/prsshini/Kaggle_WhatsCooking/master/Results/Training%20Set%20Plot.png)\r\n## Models applied on Whatscooking Data:\r\n### RPART: \r\nAs always, started with basic decision tree which can perform only upto ~60% Accuracy. Functions were used to find the best cp values. But The complexity parameter was found to be too low. On risk of overfitting the model, moved to next model...  \r\n\r\n![](https://raw.githubusercontent.com/prsshini/Kaggle_WhatsCooking/master/Results/WCDTPlot.png)\r\n###Random Forest: \r\nWhen applying the random forest, the performance did not do better than rpart. Accuracy was still less than 60%..Maybe RF is not for this dataset. so why not move to next.\r\n\r\n###XGBoost: \r\nXGBoost actually did very good on this data. It took me from 60% to 79.4%. I can easily move my position in kaggle from 150's to 50th position in the leaderboard. So I decided to stop here and move to a  new competition to get more experience on a different dataset.\r\n\r\n\r\n##Conclusion:\r\nThough I've mentioned few models above, I did still play with different model with different parameters like nnet, multinom, etc... which gave me a good experience on this multiclass prediction. \r\n\r\n### Authors and Contributors\r\nYou can visit my GitHub repository for this project to get a more insight of scripts and results. @prsshini  \r\n\r\n### Support or Contact\r\nHaving trouble with Pages? Check out our [documentation](https://help.github.com/pages) or [contact support](https://github.com/contact) and we’ll help you sort it out.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}